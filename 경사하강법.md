## 경사하강법

### 손실 함수 시각화

```python
import tensorflow as tf
import matplotlib.pyplot as plt
X = [1, 2, 3]
Y = [1, 2, 3]
W = tf.placeholder(tf.float32)
hypothesis = X * W # H(x) = Wx
cost = tf.reduce_mean(tf.square(hypothesis - Y))
sess = tf.Session()
sess.run(tf.global_variables_initializer()) 
W_val = []
cost_val = []
for i in range(-30, 50):
    feed_W = i * 0.1
    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})
    W_val.append(curr_W)
    cost_val.append(curr_cost)

plt.plot(W_val, cost_val)
plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/91046182-f4fab800-e652-11ea-9967-c986724b1238.JPG" width=55% />

<img src="https://user-images.githubusercontent.com/58063806/91045403-c6c8a880-e651-11ea-97fe-331f25d4b036.JPG" width=55% />

위와 같은 형태로 나타나는 손실함수의 **cost가 0이 되는 부분**을 찾아야 함

**위치한 부분의 경사를 따라서 일정수준 이동하는 것을 반복**하면 **경사가 0이 되는 부분에 다다름**

<img src="https://user-images.githubusercontent.com/58063806/91046185-f5934e80-e652-11ea-9d8e-3014fa9aa915.JPG" style="zoom:50%;" />

**W(최적) = W - 학습률(learning rate) X 손실함수 미분**

```python
import tensorflow as tf
X = [1, 2, 3]
Y = [1, 2, 3]
W = tf.Variable(5.)
hypothesis = X * W
cost = tf.reduce_mean(tf.square(hypothesis - Y))
gradient = tf.reduce_mean((W * X - Y) * X) * 2
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
gvs = optimizer.compute_gradients(cost) # 기울기 계산
apply_gradients = optimizer.apply_gradients(gvs) # 기울기를 적용
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for step in range(100):
    print(step, sess.run([gradient, W, gvs])) 
    sess.run(apply_gradients)
```

<img src="https://user-images.githubusercontent.com/58063806/91047929-d34f0000-e655-11ea-9945-dc3e789277ef.JPG" width=50% />

<img src="https://user-images.githubusercontent.com/58063806/91047931-d3e79680-e655-11ea-9e18-1621cc26f7c3.JPG" width=30% />

위의 결과를 보면 **수식적으로 구현한 결과와 tensorflow가 자동으로 계산한 값이 거의 일치**하는 것을 볼 수 있음